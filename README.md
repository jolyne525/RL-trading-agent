# üìà Reinforcement Learning for Algorithmic Trading (DQN)

[![GitHub Repo](https://img.shields.io/badge/GitHub-Repo-blue?logo=github)](https://github.com/jolyne525/RL-trading-agent)
[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://RL-trading-agent-bpgaqfvcpgg2tnc7mhlxga.streamlit.app/)

This project formulates single-asset trading as a **Markov Decision Process (MDP)** and trains a **Deep Q-Network (DQN)** agent on historical daily prices. It provides:

* A **headless rltrader** (`rltrader.py`) for reproducible training + walk-forward backtesting
* A **Streamlit dashboard** (`app.py`) for interactive analysis (signals, equity curves, metrics)
* A **CLI batch runner** (`run_experiments.py`) that exports results to CSV/JSON under `results/`

**Tech Stack:** Python (NumPy, pandas), Streamlit, Plotly, yfinance

> Note: research/education demo only. Not financial advice.

---

## üñºÔ∏è Demo Preview

<p align="center">
  <img src="https://github.com/user-attachments/assets/37c6c58c-21b3-4c3d-944e-dea37b364258" alt="RL Trading Agent Dashboard" width="900">
</p>

---
## ‚ú® What This Project Demonstrates 

* **MDP Modeling (Trading as Decision Process)**
  Compact state representation with discrete actions (hold/buy/sell), and a reward derived from **Œî(net worth)**.

* **Canonical DQN Components (Implemented from scratch in NumPy)**
  **Experience replay**, **target network** (hard updates), and optional **Double DQN** target computation.

* **More Realistic Execution Model**
  Supports **fixed fee**, **proportional cost (bps)**, and **slippage (bps)**; these affect net worth and therefore learning dynamics.

* **Walk-Forward Train/Test Evaluation**
  Chronological split (train on early period, test on later period) to reduce look-ahead bias.

* **Quant Metrics + Benchmarking**
  Compares against **Buy & Hold** and reports **cumulative return**, **Sharpe (annualized)**, **max drawdown**, **turnover**, and **alpha vs benchmark**.

* **Interactive Dashboard + Batch Experiments**
  Streamlit UI for analysis; CLI runner for reproducible CSV/JSON artifacts under `results/`.

---

## üîß Method Overview

### 1) MDP Design

* **State:** `[daily return, position flag, bias]` (3D)

* **Action space:** `{0: hold, 1: buy, 2: sell}`

* **Execution & costs:**
  Buy executes at `price*(1+slippage)`; sell at `price*(1-slippage)`.
  Transaction cost = `fixed_cost + (cost_bps/10000)*notional`.

* **Reward:**
  `reward = Œî(net worth) * reward_scale`
  (net worth already reflects costs/slippage through balance and executed trades)

### 2) DQN Training

* **Exploration:** epsilon-greedy with optional linear decay
* **Learning:** TD loss with discount factor `gamma`
* **Replay Buffer:** uniform sampling
* **Target Network:** hard update every N steps
* **Double DQN:** optional (online selects action, target evaluates)

### 3) Evaluation (Walk-forward)

* Chronological split by `train_ratio` (default 0.7)
* Test uses greedy policy (`eval_mode=True`)
* Benchmark: Buy & Hold NAV

---

## üöÄ Quick Start (Run Locally)

### 0) Requirements

* **Python ‚â• 3.10** (required by type syntax like `str | None`)

### 1) Clone

```bash
git clone https://github.com/jolyne525/RL-trading-agent.git
cd RL-trading-agent
```

### 2) Install Dependencies

Recommended: use a virtual environment.

```bash
python -m venv .venv
# Windows (PowerShell)
# .\.venv\Scripts\Activate.ps1
# macOS/Linux
source .venv/bin/activate

pip install -r requirements.txt
```

### 3) Run the Streamlit App

```bash
streamlit run app.py
```

Open the URL shown in the terminal (usually [http://localhost:8501](http://localhost:8501)).

---

## üß™ Run Batch Experiments (Headless CLI)

This generates reproducible artifacts under `results/`.

```bash
python run_experiments.py \
  --tickers NVDA,AAPL \
  --start 2021-01-01 \
  --end 2021-06-01 \
  --episodes 200 \
  --seed 42
```

Optional friction controls:

```bash
python run_experiments.py --tickers NVDA --start 2021-01-01 --end 2021-06-01 \
  --episodes 200 --seed 42 \
  --fixed_cost 0.05 --cost_bps 1.0 --slippage_bps 1.0
```

---

## üì¶ Results (Out-of-sample)

Metrics are computed on the **walk-forward test split** and benchmarked against **Buy & Hold**.  
All reported values come from `results/summary.csv` and the corresponding test equity curves.

<!-- Paste the table generated by: python scripts/make_readme_results.py -->
<!-- BEGIN RESULTS TABLE -->
(paste results/results_table.md here)
<!-- END RESULTS TABLE -->

Equity curves on the test split:

![AAPL equity](results/AAPL_equity.png)
![NVDA equity](results/NVDA_equity.png)

Notes:
- Alpha = Strategy return ‚àí Buy&Hold return on the same test window.
- Sharpe is annualized from daily returns (252 trading days assumption).
- Turnover is notional traded / initial balance (proxy for trading intensity).

---

## Reproducibility

The exact run configuration is stored in `results/*_run_config.json`.  
To reproduce the reported results (same window / seed / hyperparameters), run:

```bash
python run_experiments.py --tickers AAPL,NVDA --start 2016-01-01 --end 2024-01-01 \
  --episodes 1200 --seed 1 --out_dir results --data_source stooq --cache_dir data_cache \
  --double_dqn --buffer_size 100000 --batch_size 64 --min_buffer_size 256 --target_update_every 500 \
  --learning_rate 0.0003 --gamma 0.99 --epsilon_end 0.10 --epsilon_decay_steps 80000 \
  --fixed_cost 0.05 --cost_bps 1 --slippage_bps 1

---

## üìÇ Project Structure

```text
RL-trading-agent/
  rltrader/
    __init__.py            # Public API exports
    config.py              # Dataclasses for env/DQN/backtest configs
    utils.py               # Seed & utility helpers
    data.py                # Data ingestion (stooq/yfinance + caching)
    env.py                 # Trading MDP environment (costs/slippage)
    agent.py               # DQN agent (replay, target net, optional Double DQN)
    metrics.py             # Performance metrics (Sharpe, MDD, alpha, turnover)
    train_eval.py          # Walk-forward training + evaluation pipeline
  app.py                   # Streamlit dashboard
  run_experiments.py       # CLI runner -> results/*.csv + *.json
  scripts/                 # Helper scripts (plot, README table generation)
  requirements.txt
  README.md
  .gitignore
  results/                 # Selected artifacts for README (optional to commit)
  data_cache/              # Local price cache (recommended gitignore)
```

---

## ‚ö†Ô∏è Notes / Limitations

* Research/education demo, not trading advice.
* Single-asset, simplified state and discrete actions; position sizing is fixed (shares per trade).
* Results can be sensitive to regime, date window, and hyperparameters.
* Execution is simplified (fixed + proportional cost + slippage model), not a full microstructure simulator.

---

## üìú License

MIT

---